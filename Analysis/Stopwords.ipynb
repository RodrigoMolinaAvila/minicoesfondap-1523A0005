{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return \" \".join(re.sub(r\"[0-9]\", \" \", text).split())\n",
    "\n",
    "def remove_unprintable_(text):\n",
    "    printable = set(string.printable + \"ñáéíóúüÑÁÉÍÓÚÜ\")\n",
    "    return \"\".join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    pattern = re.compile(r\"[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]\")\n",
    "    return re.sub(\" +\", \" \", pattern.sub(\" \", text))\n",
    "\n",
    "def reduce_spam(text):\n",
    "    text = re.sub(r\"(\\w+)(\\s+\\1){2,}\", r\"\\1\", text)\n",
    "    text = re.sub(r\"(\\w+\\s+\\w+)(\\s+\\1){2,}\", r\"\\1\", text)\n",
    "    return text\n",
    "\n",
    "def remove_vowels_accents(text):\n",
    "    return (\n",
    "        text.replace(\"á\", \"a\").replace(\"é\", \"e\")\n",
    "            .replace(\"í\", \"i\").replace(\"ó\", \"o\")\n",
    "            .replace(\"ú\", \"u\").replace(\"ü\", \"u\")\n",
    "    )\n",
    "\n",
    "def remove_stopwords(text, stopwords_list):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords_list])\n",
    "\n",
    "def clean_text(text, stopwords_list):\n",
    "    text = text.lower()\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_unprintable_(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = reduce_spam(text)\n",
    "    text = remove_stopwords(text, stopwords_list)\n",
    "    text = remove_vowels_accents(text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del corpus desde Excel (ajusta el path)\n",
    "df = pd.read_excel(\"01_bbdd_think_tanks.xlsx\")\n",
    "\n",
    "# Filtrado de fechas (si aplica)\n",
    "df['FechaPublicación'] = pd.to_datetime(df['FechaPublicación'], errors='coerce')\n",
    "df = df.dropna(subset=['Corpus'])\n",
    "\n",
    "# Definición inicial de stopwords vacías (solo para limpieza léxica)\n",
    "stopwords_list = []\n",
    "\n",
    "# Aplicar limpieza\n",
    "df['Corpus_Limpio'] = df['Corpus'].apply(lambda x: clean_text(str(x), stopwords_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra</th>\n",
       "      <th>Frecuencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de</td>\n",
       "      <td>1148144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>la</td>\n",
       "      <td>702622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>que</td>\n",
       "      <td>538062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>493543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>el</td>\n",
       "      <td>487584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>derecho</td>\n",
       "      <td>12704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>dos</td>\n",
       "      <td>12617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>durante</td>\n",
       "      <td>12539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>proceso</td>\n",
       "      <td>12493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>estas</td>\n",
       "      <td>12381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Palabra  Frecuencia\n",
       "0        de     1148144\n",
       "1        la      702622\n",
       "2       que      538062\n",
       "3        en      493543\n",
       "4        el      487584\n",
       "..      ...         ...\n",
       "95  derecho       12704\n",
       "96      dos       12617\n",
       "97  durante       12539\n",
       "98  proceso       12493\n",
       "99    estas       12381\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenización global del corpus\n",
    "tokens = \" \".join(df[\"Corpus_Limpio\"]).split()\n",
    "\n",
    "# Conteo de frecuencia\n",
    "frecuencias = Counter(tokens)\n",
    "frecuencia_df = pd.DataFrame(frecuencias.items(), columns=[\"Palabra\", \"Frecuencia\"])\n",
    "frecuencia_df = frecuencia_df.sort_values(by=\"Frecuencia\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Visualizar top-N palabras\n",
    "frecuencia_df.head(100)  # Puedes ajustar el N según necesidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_palabra_en_documentos(palabra_objetivo, dataframe, columna_corpus):\n",
    "    palabra_objetivo = palabra_objetivo.lower()\n",
    "    documentos = dataframe[dataframe[columna_corpus].str.contains(rf\"\\b{palabra_objetivo}\\b\", na=False, case=False)]\n",
    "    return documentos\n",
    "\n",
    "# Ejemplo de uso\n",
    "palabra_objetivo = \"economía\"  # Cambia esta palabra según lo que quieras buscar\n",
    "documentos_con_palabra = buscar_palabra_en_documentos(palabra_objetivo, df, \"Corpus_Limpio\")\n",
    "\n",
    "# Mostrar los documentos encontrados\n",
    "print(f\"Documentos que contienen la palabra '{palabra_objetivo}':\")\n",
    "print(documentos_con_palabra[[\"Título\", \"FechaPublicación\", \"Corpus\"]])  # Ajusta las columnas según tu DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecuencia_df.to_csv(\"frecuencia_palabras.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords ampliadas generadas: 98 palabras\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# --- Cargar modelo grande de spaCy ---\n",
    "nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "# --- Leer palabras más frecuentes ---\n",
    "df = pd.read_csv(\"frecuencia_palabras.csv\")\n",
    "top_words = df.head(1000)[\"Palabra\"].dropna().astype(str).tolist()\n",
    "text = \" \".join(top_words)\n",
    "doc = nlp(text)\n",
    "\n",
    "# --- A. POS estructurales vacíos ---\n",
    "tags_pos_funcionales = {\"DET\", \"ADP\", \"PRON\", \"CCONJ\", \"SCONJ\", \"AUX\", \"PART\"}\n",
    "\n",
    "stopwords_pos = {\n",
    "    token.lemma_.lower()\n",
    "    for token in doc\n",
    "    if token.pos_ in tags_pos_funcionales\n",
    "}\n",
    "\n",
    "# --- B. Funciones sintácticas vacías (dependencias) ---\n",
    "deps_funcionales = {\"aux\", \"det\", \"mark\", \"cc\", \"case\", \"cop\", \"punct\"}\n",
    "\n",
    "stopwords_dep = {\n",
    "    token.lemma_.lower()\n",
    "    for token in doc\n",
    "    if token.dep_ in deps_funcionales\n",
    "}\n",
    "\n",
    "# --- C. Verbos funcionales (light verbs conocidos en español) ---\n",
    "verbos_funcionales = {\n",
    "    \"hacer\", \"realizar\", \"tener\", \"haber\", \"decir\", \"dar\", \"llevar\",\n",
    "    \"ser\", \"estar\", \"poner\", \"tomar\", \"ver\", \"pasar\"\n",
    "}\n",
    "\n",
    "stopwords_verbos = {\n",
    "    token.lemma_.lower()\n",
    "    for token in doc\n",
    "    if token.pos_ == \"VERB\" and token.lemma_.lower() in verbos_funcionales\n",
    "}\n",
    "\n",
    "# --- D. Consolidar todas las stopwords ---\n",
    "stopwords_total = sorted(stopwords_pos.union(stopwords_dep).union(stopwords_verbos))\n",
    "\n",
    "# --- Guardar a archivo ---\n",
    "with open(\"stopwords_expandida.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for palabra in stopwords_total:\n",
    "        f.write(palabra + \"\\n\")\n",
    "\n",
    "print(f\"Stopwords ampliadas generadas: {len(stopwords_total)} palabras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
