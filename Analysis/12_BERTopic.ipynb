{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3df51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ba2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"01_bbdd_think_tanks_no_stopwords.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60451768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Diagnóstico de longitud textual:\n",
    "- Cuenta de palabras por fila en df[\"Texto\"] (robusto a NaN y espacios).\n",
    "- Resumen global (percentiles) y por 'Think Tank'.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Utilitarios\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "_WORD_RGX = re.compile(r\"[A-Za-zÁÉÍÓÚáéíóúÑñÜü0-9]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def _count_words(text: Optional[str]) -> int:\n",
    "    \"\"\"Cuenta 'palabras' con regex Unicode (letras con tildes y dígitos).\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return 0\n",
    "    return len(_WORD_RGX.findall(text))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Diagnóstico principal\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "def diagnose_text_lengths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve un DataFrame con las longitudes por documento y\n",
    "    emite resúmenes globales y por Think Tank.\n",
    "    \"\"\"\n",
    "    cols_required = {\"ID\", \"Think Tank\", \"Texto\"}\n",
    "    missing = cols_required - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Faltan columnas requeridas: {missing}\")\n",
    "\n",
    "    out = df[[\"ID\", \"Think Tank\", \"Texto\"]].copy()\n",
    "    out[\"word_count\"] = out[\"Texto\"].apply(_count_words).astype(int)\n",
    "    out[\"char_count\"] = out[\"Texto\"].astype(str).str.len().astype(int)\n",
    "\n",
    "    # Resumen global\n",
    "    q = out[\"word_count\"].quantile([0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "    print(\"\\n[Resumen global word_count]\")\n",
    "    print(f\"n={len(out)} | mean={out['word_count'].mean():.1f} | \"\n",
    "          f\"std={out['word_count'].std(ddof=1):.1f} | \"\n",
    "          f\"min={out['word_count'].min()} | \"\n",
    "          f\"p50={q.loc[0.5]:.0f} | p75={q.loc[0.75]:.0f} | \"\n",
    "          f\"p90={q.loc[0.9]:.0f} | p95={q.loc[0.95]:.0f} | \"\n",
    "          f\"p99={q.loc[0.99]:.0f} | max={out['word_count'].max()}\")\n",
    "\n",
    "    # Resumen por Think Tank (media, mediana, p95 y n)\n",
    "    grp = (\n",
    "        out.groupby(\"Think Tank\")[\"word_count\"]\n",
    "        .agg(n=\"count\",\n",
    "             mean=\"mean\",\n",
    "             p50=lambda s: s.quantile(0.5),\n",
    "             p95=lambda s: s.quantile(0.95),\n",
    "             max=\"max\")\n",
    "        .sort_values(by=\"mean\", ascending=False)\n",
    "    )\n",
    "    # Formateo rápido para lectura\n",
    "    grp_fmt = grp.copy()\n",
    "    for c in [\"mean\", \"p50\", \"p95\"]:\n",
    "        grp_fmt[c] = grp_fmt[c].round(1)\n",
    "\n",
    "    print(\"\\n[Resumen por Think Tank] (ordenado por media de palabras)\")\n",
    "    print(grp_fmt)\n",
    "\n",
    "    # Muestras extremas (opcional, útiles para inspección manual)\n",
    "    print(\"\\n[Ejemplos con textos más largos]\")\n",
    "    print(out.nlargest(5, \"word_count\")[[\"ID\", \"Think Tank\", \"word_count\"]])\n",
    "\n",
    "    print(\"\\n[Ejemplos con textos más cortos]\")\n",
    "    print(out.nsmallest(5, \"word_count\")[[\"ID\", \"Think Tank\", \"word_count\"]])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Uso\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Ejecuta:\n",
    "# lengths_df = diagnose_text_lengths(df)\n",
    "# lengths_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45ab450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Resumen global word_count]\n",
      "n=17432 | mean=533.9 | std=672.4 | min=0 | p50=384 | p75=641 | p90=1042 | p95=1440 | p99=4518 | max=6691\n",
      "\n",
      "[Resumen por Think Tank] (ordenado por media de palabras)\n",
      "                           n    mean     p50     p95   max\n",
      "Think Tank                                                \n",
      "CDC                      171  3248.4  3480.0  5300.0  6527\n",
      "IES                       20  3206.2  4996.0  5223.8  5259\n",
      "Idea País                  3  3053.7  4014.0  5031.9  5145\n",
      "ICAL                      38  1086.8   911.5  3043.4  3190\n",
      "Nodo XXI                 328   871.4   709.0  2105.0  5089\n",
      "Instituto Res Pública    575   782.2   614.0  1697.2  4913\n",
      "Casa Común               227   728.8   501.0  2392.6  5554\n",
      "FPP                     2586   680.3   432.5  2037.0  5562\n",
      "Fundación Sol            550   637.8   569.5  1688.3  5128\n",
      "LyD                     2623   569.3   496.0  1317.8  6691\n",
      "CEP                     2008   553.4   492.0  1305.5  5700\n",
      "Horizonte Ciudadano      130   489.4   495.0  1042.8  3177\n",
      "Instituto Igualdad       892   442.7   207.5  1234.9  5116\n",
      "CLAPES UC                207   419.4   286.0  1042.2  3394\n",
      "OPES                      45   415.8   170.0  1279.6  2703\n",
      "Pivotes                  412   414.5   294.0   990.7  2670\n",
      "CED                      265   395.4   305.0   993.8  2134\n",
      "Espacio Público         1525   386.0   409.0   952.6  2220\n",
      "Signos Uandes           3612   345.4   290.0   775.4  4567\n",
      "Fundación Jaime Guzmán   678   331.2   176.0  1101.2  5291\n",
      "Chile 21                 288   270.2    10.0  2033.0  5490\n",
      "Instituto Libertad       124   193.4   134.0   456.9   698\n",
      "Horizontal               116   161.3   114.5   364.2   451\n",
      "Ideas Republicanas         9   101.9   104.0   126.6   127\n",
      "\n",
      "[Ejemplos con textos más largos]\n",
      "          ID Think Tank  word_count\n",
      "7444    7445        LyD        6691\n",
      "7438    7439        LyD        6592\n",
      "18363  18360        CDC        6527\n",
      "18398  18395        CDC        6323\n",
      "18437  18434        CDC        6070\n",
      "\n",
      "[Ejemplos con textos más cortos]\n",
      "          ID              Think Tank  word_count\n",
      "11609  11612         Espacio Público           0\n",
      "11610  11612         Espacio Público           0\n",
      "11613  11615         Espacio Público           0\n",
      "11614  11615         Espacio Público           0\n",
      "25731  25728  Fundación Jaime Guzmán           1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Think Tank</th>\n",
       "      <th>Texto</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>206</td>\n",
       "      <td>LyD</td>\n",
       "      <td>MERCADO LABORAL PERMANECE DÉBIL A PESAR DE LA ...</td>\n",
       "      <td>446</td>\n",
       "      <td>2689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>207</td>\n",
       "      <td>LyD</td>\n",
       "      <td>ENCUESTA DE PERCEPCIÓN DEL EMPLEO LYD DICIEMBR...</td>\n",
       "      <td>324</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>208</td>\n",
       "      <td>LyD</td>\n",
       "      <td>ESTUDIO LYD: HOSPITALES ESTATALES PODRÍAN AUME...</td>\n",
       "      <td>695</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>209</td>\n",
       "      <td>LyD</td>\n",
       "      <td>VALIOSOS INSUMOS TÉCNICOS PARA RESOLVER LA CRI...</td>\n",
       "      <td>471</td>\n",
       "      <td>2967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>210</td>\n",
       "      <td>LyD</td>\n",
       "      <td>INFLACIÓN ANUAL CONTINÚA A LA BAJA. El Institu...</td>\n",
       "      <td>333</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Think Tank                                              Texto  \\\n",
       "205  206        LyD  MERCADO LABORAL PERMANECE DÉBIL A PESAR DE LA ...   \n",
       "206  207        LyD  ENCUESTA DE PERCEPCIÓN DEL EMPLEO LYD DICIEMBR...   \n",
       "207  208        LyD  ESTUDIO LYD: HOSPITALES ESTATALES PODRÍAN AUME...   \n",
       "208  209        LyD  VALIOSOS INSUMOS TÉCNICOS PARA RESOLVER LA CRI...   \n",
       "209  210        LyD  INFLACIÓN ANUAL CONTINÚA A LA BAJA. El Institu...   \n",
       "\n",
       "     word_count  char_count  \n",
       "205         446        2689  \n",
       "206         324        1899  \n",
       "207         695        4332  \n",
       "208         471        2967  \n",
       "209         333        1959  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_df = diagnose_text_lengths(df)\n",
    "lengths_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379b959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17432 entries, 205 to 31640\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   ID                    17432 non-null  int64         \n",
      " 1   Think Tank            17432 non-null  object        \n",
      " 2   Tipo de Think Tank    17432 non-null  object        \n",
      " 3   Orientación Política  17432 non-null  object        \n",
      " 4   Autor                 8390 non-null   object        \n",
      " 5   Título                17425 non-null  object        \n",
      " 6   Medio                 2498 non-null   object        \n",
      " 7   Corpus                16690 non-null  object        \n",
      " 8   Producto              10353 non-null  object        \n",
      " 9   Enlace                17432 non-null  object        \n",
      " 10  CorpusPDF             237 non-null    object        \n",
      " 11  FechaPublicacion      17432 non-null  datetime64[ns]\n",
      " 12  Año                   17432 non-null  float64       \n",
      " 13  Mes                   17432 non-null  float64       \n",
      " 14  Dia                   17432 non-null  float64       \n",
      " 15  Texto                 17432 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(3), int64(1), object(11)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pipeline CPU (completo) con cobertura 100% y App Dash.\n",
    "\n",
    "Requiere columnas en df:\n",
    "['ID', 'Think Tank', 'Texto', 'FechaPublicacion', 'Enlace']\n",
    "\n",
    "Hace:\n",
    "- SBERT (paraphrase-multilingual-mpnet-base-v2)\n",
    "- BERTopic (macro-tópicos)\n",
    "- Post-proceso de ruido: micro-tópicos (kNN + umbral coseno) y singletons\n",
    "- Keywords por cluster (macro: BERTopic; micro/singleton: TF-IDF local)\n",
    "- UMAP 2D/3D\n",
    "- App Dash (toggle color ThinkTank/Tópico; histograma mensual por ThinkTank; tabla con enlace clickeable)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, re, json, sys, math, hashlib, colorsys, platform\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional, Sequence\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "@dataclass\n",
    "class Config:\n",
    "    # columnas\n",
    "    id_col: str = \"ID\"\n",
    "    cat_col: str = \"Think Tank\"\n",
    "    text_col: str = \"Texto\"\n",
    "    date_col: str = \"FechaPublicacion\"\n",
    "    url_col: str = \"Enlace\"\n",
    "\n",
    "    # filtro temporal (tú puedes setearlos antes de llamar run_pipeline_on_df)\n",
    "    date_start: Optional[pd.Timestamp] = None\n",
    "    date_end: Optional[pd.Timestamp] = None  # exclusivo\n",
    "\n",
    "    # rutas\n",
    "    out_dir_root: str = \"./NLP\"\n",
    "    run_id: Optional[str] = None\n",
    "    run_dir: Optional[str] = None\n",
    "    out_dir_topics: str = \"\"\n",
    "    out_dir_map: str = \"\"\n",
    "\n",
    "    # SBERT\n",
    "    sbert_model_name: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    sbert_batch_size: int = 32\n",
    "    # sin cache: reconstruimos siempre (como pediste)\n",
    "    force_reembed: bool = True\n",
    "\n",
    "    # Limpieza ligera\n",
    "    min_token_len: int = 3\n",
    "\n",
    "    # UMAP pro-diversidad\n",
    "    umap_neighbors_grid: Tuple[int, ...] = (10, 15, 20)\n",
    "    umap_min_dist_grid: Tuple[float, ...] = (0.3, 0.5, 0.7)\n",
    "    umap_n_components: int = 10\n",
    "    umap_metric: str = \"cosine\"\n",
    "    random_state: int = 42\n",
    "\n",
    "    # HDBSCAN (macro)\n",
    "    hdbscan_min_cluster_size: int = 8     # más bajo → más clusters\n",
    "    hdbscan_min_samples: int = 5          # separa mejor islas\n",
    "    hdbscan_metric: str = \"euclidean\"\n",
    "\n",
    "    # Micro-tópicos sobre ruido\n",
    "    micro_knn_k: int = 10                 # vecinos para grafo\n",
    "    micro_cos_threshold: float = 0.65     # umbral coseno para unir outliers\n",
    "    accept_singletons: bool = True        # crear singleton si no tiene vecinos\n",
    "\n",
    "    # Keywords\n",
    "    topic_kw_fixed: int = 4               # fijo para la app\n",
    "    tfidf_max_features_micro: int = 1000  # para micro/singleton\n",
    "\n",
    "    # App (marcadores)\n",
    "    app_marker_3d_size: int = 3\n",
    "    app_marker_2d_size: int = 5\n",
    "\n",
    "    # Vectorizer para BERTopic (control de extremos)\n",
    "    vec_min_df_abs: int = 2               # al menos en 2 docs\n",
    "    vec_min_df_frac: float = 0.002        # o 0.2% del corpus\n",
    "    vec_max_df: float = 0.75              # ignora super frecuentes\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def _log(msg: str):\n",
    "    if CFG.verbose:\n",
    "        ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"[{ts}] {msg}\")\n",
    "\n",
    "def start_run_dirs(cfg: Config) -> None:\n",
    "    Path(cfg.out_dir_root).mkdir(parents=True, exist_ok=True)\n",
    "    runs_root = Path(cfg.out_dir_root) / \"runs\"\n",
    "    runs_root.mkdir(exist_ok=True, parents=True)\n",
    "    cfg.run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    cfg.run_dir = str(runs_root / cfg.run_id)\n",
    "    cfg.out_dir_topics = str(Path(cfg.run_dir) / \"bertopic_outputs\")\n",
    "    cfg.out_dir_map = str(Path(cfg.run_dir) / \"bertopic_mapping_outputs\")\n",
    "    Path(cfg.out_dir_topics).mkdir(parents=True, exist_ok=True)\n",
    "    Path(cfg.out_dir_map).mkdir(parents=True, exist_ok=True)\n",
    "    _log(f\"[RUN] id={cfg.run_id} -> {cfg.run_dir}\")\n",
    "\n",
    "def scrub_text(s: str, min_len: int = 3) -> str:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return \"\"\n",
    "    t = s\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"\\b\\d+\\b\", \" \", t)\n",
    "    t = re.sub(r\"[^\\w\\sÁÉÍÓÚáéíóúÑñÜü-]\", \" \", t, flags=re.UNICODE)\n",
    "    t = \" \".join([w for w in t.split() if len(w) >= min_len])\n",
    "    return t.strip()\n",
    "\n",
    "def cosine_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    a_n = sk_normalize(a)\n",
    "    b_n = sk_normalize(b)\n",
    "    return np.clip(a_n @ b_n.T, -1.0, 1.0)\n",
    "\n",
    "def month_str(dt: pd.Timestamp) -> str:\n",
    "    if pd.isna(dt): return \"NA\"\n",
    "    return f\"{dt.year:04d}-{dt.month:02d}\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Embeddings (reconstruir siempre)\n",
    "# =========================\n",
    "def compute_embeddings(df_local: pd.DataFrame, cfg: Config) -> np.ndarray:\n",
    "    _log(\"Calculando embeddings SBERT…\")\n",
    "    model = SentenceTransformer(cfg.sbert_model_name)\n",
    "    texts = df_local[cfg.text_col].astype(str).map(lambda x: scrub_text(x, cfg.min_token_len)).tolist()\n",
    "    emb = model.encode(\n",
    "        texts, batch_size=cfg.sbert_batch_size,\n",
    "        show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True\n",
    "    )\n",
    "    return sk_normalize(emb.astype(np.float32))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# BERTopic (macro)\n",
    "# =========================\n",
    "def make_topic_model(n_docs: int, cfg: Config, n_neighbors: int, min_dist: float) -> BERTopic:\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors, min_dist=min_dist, n_components=cfg.umap_n_components,\n",
    "        metric=cfg.umap_metric, random_state=cfg.random_state, verbose=True\n",
    "    )\n",
    "    # vectorizer seguro\n",
    "    min_df_abs = cfg.vec_min_df_abs\n",
    "    min_df_frac = cfg.vec_min_df_frac\n",
    "    min_df = max(min_df_abs, int(max(1, n_docs * min_df_frac)))\n",
    "    vec = CountVectorizer(\n",
    "        lowercase=True, ngram_range=(1,2), min_df=min_df, max_df=cfg.vec_max_df,\n",
    "        token_pattern=r\"(?u)[A-Za-zÁÉÍÓÚáéíóúÑñÜü]{3,}\"\n",
    "    )\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=cfg.hdbscan_min_cluster_size,\n",
    "        min_samples=cfg.hdbscan_min_samples,\n",
    "        metric=cfg.hdbscan_metric, prediction_data=True, cluster_selection_method=\"eom\"\n",
    "    )\n",
    "    tm = BERTopic(\n",
    "        embedding_model=None, umap_model=reducer, hdbscan_model=clusterer,\n",
    "        language=\"multilingual\", calculate_probabilities=True,\n",
    "        vectorizer_model=vec, top_n_words=max(cfg.topic_kw_fixed, 10),\n",
    "        verbose=True\n",
    "    )\n",
    "    return tm\n",
    "\n",
    "def fixed_keywords_macro(topic_model: BERTopic, k: int) -> Dict[int, List[str]]:\n",
    "    info = topic_model.get_topic_info()\n",
    "    valid = [int(t) for t in info[\"Topic\"].tolist() if int(t) != -1]\n",
    "    out: Dict[int, List[str]] = {}\n",
    "    for t in valid:\n",
    "        terms = [w for (w, _) in (topic_model.get_topic(int(t)) or [])][:k]\n",
    "        out[int(t)] = terms\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Micro-tópicos + singletons (sobre ruido)\n",
    "# =========================\n",
    "def build_micro_clusters(outlier_idx: np.ndarray, emb: np.ndarray, cfg: Config) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Crea micro-clusters por proximidad coseno. Retorna dict {cluster_id_interno: [rows]}.\n",
    "    \"\"\"\n",
    "    if outlier_idx.size == 0:\n",
    "        return {}\n",
    "    X = emb[outlier_idx]\n",
    "    # kNN en coseno → usamos métrica euclidean sobre vectores normalizados ≈ coseno\n",
    "    nn = NearestNeighbors(n_neighbors=min(cfg.micro_knn_k, max(2, len(outlier_idx))), metric=\"cosine\")\n",
    "    nn.fit(X)\n",
    "    dists, neigh = nn.kneighbors(X, return_distance=True)\n",
    "    # convertimos a similitud coseno\n",
    "    sims = 1.0 - dists\n",
    "    used = np.zeros(len(outlier_idx), dtype=bool)\n",
    "    clusters: Dict[int, List[int]] = {}\n",
    "    cid = 0\n",
    "    for i in range(len(outlier_idx)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        # vecinos sobre umbral\n",
    "        mates = [i]\n",
    "        for j, s in zip(neigh[i][1:], sims[i][1:]):  # omitir self\n",
    "            if s >= cfg.micro_cos_threshold:\n",
    "                mates.append(j)\n",
    "        if len(mates) >= 2:\n",
    "            used[mates] = True\n",
    "            clusters[cid] = [int(outlier_idx[m]) for m in mates]\n",
    "            cid += 1\n",
    "        else:\n",
    "            # potencial singleton\n",
    "            if cfg.accept_singletons:\n",
    "                used[i] = True\n",
    "                clusters[cid] = [int(outlier_idx[i])]\n",
    "                cid += 1\n",
    "            else:\n",
    "                # se deja sin cluster (seguiría como -1)\n",
    "                pass\n",
    "    return clusters\n",
    "\n",
    "def tfidf_keywords_for_docs(texts: List[str], k: int, max_features: int=1000) -> List[str]:\n",
    "    if len(texts) == 0:\n",
    "        return []\n",
    "    vec = TfidfVectorizer(\n",
    "        lowercase=True, ngram_range=(1,2), max_features=max_features,\n",
    "        token_pattern=r\"(?u)[A-Za-zÁÉÍÓÚáéíóúÑñÜü]{3,}\"\n",
    "    )\n",
    "    X = vec.fit_transform(texts)\n",
    "    if X.shape[0] == 1:\n",
    "        # una sola doc: ordenar por TF-IDF de esa fila\n",
    "        row = X[0].toarray().ravel()\n",
    "        idx = row.argsort()[::-1]\n",
    "        vocab = np.array(vec.get_feature_names_out())\n",
    "        return vocab[idx][:k].tolist()\n",
    "    else:\n",
    "        row = np.asarray(X.sum(axis=0)).ravel()\n",
    "        idx = row.argsort()[::-1]\n",
    "        vocab = np.array(vec.get_feature_names_out())\n",
    "        return vocab[idx][:k].tolist()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# App writer\n",
    "# =========================\n",
    "def write_dash_app(df_vis: pd.DataFrame, kw_by_topic: Dict[int, List[str]], run_dir: str, kw_fixed: int,\n",
    "                   size2d: int, size3d: int) -> None:\n",
    "    out_dir = Path(run_dir) / \"bertopic_outputs\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Datos\n",
    "    data_csv = out_dir / \"df_vis.csv\"\n",
    "    # FECHA **normalizada completa** YYYY-MM-DD 00:00:00\n",
    "    df_tmp = df_vis.copy()\n",
    "    df_tmp[\"FechaPublicacion\"] = pd.to_datetime(df_tmp[\"FechaPublicacion\"], errors=\"coerce\").dt.floor(\"D\")\n",
    "    df_tmp[\"FechaPublicacion\"] = df_tmp[\"FechaPublicacion\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df_tmp.to_csv(data_csv, index=False)\n",
    "\n",
    "    # Keywords por tópico\n",
    "    kw_json = {int(k): list(v) for k,v in kw_by_topic.items()}\n",
    "    kw_path = out_dir / \"keywords_by_topic.json\"\n",
    "    with open(kw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(kw_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Colores por tópico (paleta simple HLS)\n",
    "    topic_ids_all = sorted({int(t) for t in df_vis[\"topic_id\"].unique().tolist()})\n",
    "    if len(topic_ids_all) == 0:\n",
    "        topic_ids_all = [0]\n",
    "    cols = []\n",
    "    n = max(1, len(topic_ids_all))\n",
    "    for i in range(n):\n",
    "        h = (i / float(n)) % 1.0\n",
    "        r,g,b = colorsys.hls_to_rgb(h, 0.52, 0.65)\n",
    "        cols.append(\"#{0:02x}{1:02x}{2:02x}\".format(int(r*255), int(g*255), int(b*255)))\n",
    "    cmap = {str(t): c for t,c in zip(topic_ids_all, cols)}\n",
    "    cmap_path = out_dir / \"topic_colors.json\"\n",
    "    with open(cmap_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cmap, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # ALL_KW incrustado (para autocompletar)\n",
    "    all_kw = sorted({w for ks in kw_by_topic.values() for w in ks})\n",
    "    all_kw_json = json.dumps(all_kw, ensure_ascii=False)\n",
    "\n",
    "    # App\n",
    "    app_path = Path(run_dir) / \"dash_app_density.py\"\n",
    "    app_code = f'''# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "App Dash — Entradas:\n",
    "- df_vis.csv\n",
    "- keywords_by_topic.json\n",
    "- topic_colors.json\n",
    "\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dash\n",
    "from dash import Dash, dcc, html, dash_table, Input, Output, State\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Rutas (absolutas)\n",
    "DATA_CSV = Path(r\"{{data_csv.as_posix()}}\")\n",
    "KW_JSON  = Path(r\"{{kw_path.as_posix()}}\")\n",
    "CMAP_JSON= Path(r\"{{cmap_path.as_posix()}}\")\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "with open(KW_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    kw_by_topic = json.load(f)\n",
    "with open(CMAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    topic_cmap = json.load(f)\n",
    "\n",
    "# Parseo robusto de fecha\n",
    "df[\"FechaPublicacion\"] = pd.to_datetime(df[\"FechaPublicacion\"], errors=\"coerce\")\n",
    "df[\"topic_str\"] = df[\"topic_id\"].astype(str)\n",
    "\n",
    "# Opciones UI\n",
    "tt_options = sorted(df[\"ThinkTank\"].dropna().unique().tolist())\n",
    "topic_ids = sorted([int(t) for t in df[\"topic_id\"].dropna().unique().tolist()])\n",
    "topic_options = [{{\"label\": f\"Topic {{t}}\", \"value\": t}} for t in topic_ids]\n",
    "ALL_KW = json.loads({json.dumps(all_kw_json)})\n",
    "kw_options = [{{\"label\": w, \"value\": w}} for w in ALL_KW]\n",
    "\n",
    "# Día índice para slider\n",
    "tmin = df[\"FechaPublicacion\"].min()\n",
    "tmax = df[\"FechaPublicacion\"].max()\n",
    "df[\"day_idx\"] = (df[\"FechaPublicacion\"] - tmin).dt.days.astype(\"Int64\")\n",
    "min_idx, max_idx = int(df[\"day_idx\"].min()), int(df[\"day_idx\"].max())\n",
    "\n",
    "def day_to_date(i):\n",
    "    return (tmin + pd.Timedelta(days=int(i))).date()\n",
    "\n",
    "def label_marks():\n",
    "    span = max_idx - min_idx\n",
    "    step = max(1, span // 8) if span>0 else 1\n",
    "    marks = {{}}\n",
    "    for i in range(min_idx, max_idx+1, step):\n",
    "        marks[i] = dict(label=str(day_to_date(i)))\n",
    "    marks[min_idx] = dict(label=str(day_to_date(min_idx)))\n",
    "    marks[max_idx] = dict(label=str(day_to_date(max_idx)))\n",
    "    return marks\n",
    "\n",
    "def apply_filters(dff, tt_sel, topic_sel, t_range):\n",
    "    a,b = t_range\n",
    "    dff = dff[(dff[\"day_idx\"] >= a) & (dff[\"day_idx\"] <= b)]\n",
    "    if tt_sel: dff = dff[dff[\"ThinkTank\"].isin(tt_sel)]\n",
    "    if topic_sel: dff = dff[dff[\"topic_id\"].isin(topic_sel)]\n",
    "    return dff\n",
    "\n",
    "KW_FIXED = {{int(kw_fixed)}}   # fijo UI\n",
    "SIZE2D   = {{int(size2d)}}\n",
    "SIZE3D   = {{int(size3d)}}\n",
    "\n",
    "def banner_keywords(topic_sel):\n",
    "    if not topic_sel:\n",
    "        return \"Selecciona uno o más tópicos para ver sus palabras clave.\"\n",
    "    parts=[]\n",
    "    for t in sorted(set(topic_sel)):\n",
    "        ks = kw_by_topic.get(str(int(t))) or kw_by_topic.get(int(t)) or []\n",
    "        show = ks[:KW_FIXED]\n",
    "        parts.append(\"Topic {{t}}: \" + \", \".join(show))\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "def topics_matching_keyword(word):\n",
    "    if not word:\n",
    "        return []\n",
    "    q = str(word).strip().lower()\n",
    "    hits = []\n",
    "    for k,v in kw_by_topic.items():\n",
    "        words = [w.lower() for w in v]\n",
    "        if any(q == w for w in words):\n",
    "            try:\n",
    "                hits.append(int(k))\n",
    "            except:\n",
    "                pass\n",
    "    return sorted(set(hits))\n",
    "\n",
    "app = Dash(__name__)\n",
    "app.title = \"UMAP — Explorador de tópicos\"\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Label(\"Color:\"),\n",
    "            dcc.RadioItems(\n",
    "                id=\"color-mode\",\n",
    "                options=[{{\"label\":\"Think Tank\",\"value\":\"tt\"}}, {{\"label\":\"Tópico\",\"value\":\"topic\"}}],\n",
    "                value=\"tt\", inline=True\n",
    "            )\n",
    "        ], style={{\"display\":\"inline-block\",\"marginRight\":\"16px\"}}),\n",
    "        html.Div([\n",
    "            dcc.Dropdown(options=[{{\"label\": t, \"value\": t}} for t in tt_options],\n",
    "                         id=\"tt-dd\", value=[], multi=True, placeholder=\"Think Tanks (multi)\")\n",
    "        ], style={{\"display\":\"inline-block\",\"width\":\"30%\",\"marginRight\":\"8px\"}}),\n",
    "        html.Div([\n",
    "            dcc.Dropdown(options=topic_options, id=\"topic-dd\", value=[], multi=True, placeholder=\"Tópicos (multi)\")\n",
    "        ], style={{\"display\":\"inline-block\",\"width\":\"30%\",\"marginRight\":\"8px\"}}),\n",
    "        html.Div([\n",
    "            dcc.Dropdown(id=\"kw-search-dd\", options=kw_options, value=None, multi=False, searchable=True, placeholder=\"Buscar palabra de tópico\")\n",
    "        ], style={{\"display\":\"inline-block\",\"width\":\"35%\"}})\n",
    "    ], style={{\"marginBottom\":\"8px\"}}),\n",
    "\n",
    "    html.Div(id=\"kw-banner\",\n",
    "             style={{\"margin\":\"6px 0 10px 0\",\"padding\":\"6px 10px\",\n",
    "                    \"background\":\"#f6f8fa\",\"border\":\"1px solid #e5e7eb\",\n",
    "                    \"borderRadius\":\"8px\",\"fontFamily\":\"Inter,system-ui,sans-serif\"}}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Graph(id=\"umap3d-fig\", style={{\"height\":\"48vh\"}}),\n",
    "        dcc.Graph(id=\"scatter-fig\", style={{\"height\":\"48vh\"}})\n",
    "    ], style={{\"display\":\"grid\",\"gridTemplateColumns\":\"1fr 1fr\",\"gap\":\"8px\"}}),\n",
    "\n",
    "    html.Div([\n",
    "        dcc.Graph(id=\"series-tt-fig\", style={{\"height\":\"34vh\"}})\n",
    "    ], style={{\"marginTop\":\"4px\"}}),\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Button(\"Todo el periodo\", id=\"btn-all\", n_clicks=0, style={{\"marginRight\":\"8px\"}}),\n",
    "            html.Button(\"Estallido → Fin Convención\", id=\"btn-period1\", n_clicks=0, style={{\"marginRight\":\"8px\"}}),\n",
    "            html.Button(\"Proc. Const. → Plebiscito Salida\", id=\"btn-period2\", n_clicks=0),\n",
    "        ], style={{\"textAlign\":\"center\",\"marginBottom\":\"6px\"}}),\n",
    "        html.Label(\"Rango temporal\", style={{\"display\":\"block\",\"textAlign\":\"center\",\"marginBottom\":\"6px\"}}),\n",
    "        dcc.RangeSlider(id=\"time-rs\", min=min_idx, max=max_idx,\n",
    "                        step=1, value=[min_idx, max_idx],\n",
    "                        marks=label_marks(), allowCross=False, pushable=1, updatemode=\"mouseup\")\n",
    "    ], style={{\"maxWidth\":\"1100px\",\"margin\":\"12px auto\"}}),\n",
    "\n",
    "    html.H4(\"Documentos filtrados\"),\n",
    "    dash_table.DataTable(\n",
    "        id=\"result-table\",\n",
    "        columns=[\n",
    "            {{\"name\":\"topic_id\",\"id\":\"topic_id\"}},\n",
    "            {{\"name\":\"keywords\",\"id\":\"keywords\"}},\n",
    "            {{\"name\":\"ThinkTank\",\"id\":\"ThinkTank\"}},\n",
    "            {{\"name\":\"FechaPublicacion\",\"id\":\"FechaPublicacion\"}},\n",
    "            {{\"name\":\"Enlace\",\"id\":\"Enlace_markdown\",\"presentation\":\"markdown\"}}\n",
    "        ],\n",
    "        page_size=15, sort_action=\"native\", filter_action=\"native\",\n",
    "        style_table={{\"overflowX\":\"auto\"}},\n",
    "        style_cell={{\"fontFamily\":\"Inter, system-ui, sans-serif\",\"fontSize\":\"13px\",\"padding\":\"6px\"}},\n",
    "        style_header={{\"fontWeight\":\"700\"}}\n",
    "    )\n",
    "], style={{\"padding\":\"10px\"}})\n",
    "\n",
    "# Botones de rango rápido\n",
    "@app.callback(\n",
    "    Output(\"time-rs\",\"value\"),\n",
    "    Input(\"btn-all\",\"n_clicks\"),\n",
    "    Input(\"btn-period1\",\"n_clicks\"),\n",
    "    Input(\"btn-period2\",\"n_clicks\"),\n",
    "    State(\"time-rs\",\"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def quick_ranges(n_all, n_p1, n_p2, cur):\n",
    "    ctx = dash.callback_context\n",
    "    if not ctx.triggered: return cur\n",
    "    trg = ctx.triggered[0][\"prop_id\"].split(\".\")[0]\n",
    "    if trg == \"btn-all\":\n",
    "        return [min_idx, max_idx]\n",
    "    elif trg == \"btn-period1\":\n",
    "        a = int((pd.Timestamp(\"2019-10-18\") - tmin).days)\n",
    "        b = int((pd.Timestamp(\"2022-09-04\") - tmin).days)\n",
    "        return [max(min_idx,a), min(max_idx,b)]\n",
    "    elif trg == \"btn-period2\":\n",
    "        a = int((pd.Timestamp(\"2022-09-05\") - tmin).days)\n",
    "        b = int((pd.Timestamp(\"2023-12-17\") - tmin).days)\n",
    "        return [max(min_idx,a), min(max_idx,b)]\n",
    "    return cur\n",
    "\n",
    "# Autocompletar: setea tópicos que contienen la palabra\n",
    "@app.callback(\n",
    "    Output(\"topic-dd\",\"value\"),\n",
    "    Input(\"kw-search-dd\",\"value\"),\n",
    "    State(\"topic-dd\",\"value\"),\n",
    "    prevent_initial_call=True\n",
    ")\n",
    "def on_kw_select(word, cur_sel):\n",
    "    hits = topics_matching_keyword(word)\n",
    "    if hits:\n",
    "        return hits\n",
    "    return cur_sel or []\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"umap3d-fig\",\"figure\"),\n",
    "    Output(\"scatter-fig\",\"figure\"),\n",
    "    Output(\"series-tt-fig\",\"figure\"),\n",
    "    Output(\"result-table\",\"data\"),\n",
    "    Output(\"kw-banner\",\"children\"),\n",
    "    Input(\"color-mode\",\"value\"),\n",
    "    Input(\"tt-dd\",\"value\"), Input(\"topic-dd\",\"value\"), Input(\"time-rs\",\"value\")\n",
    ")\n",
    "def update_all(color_mode, tt_sel, topic_sel, t_range):\n",
    "    tt_sel = tt_sel or []\n",
    "    topic_sel = topic_sel or []\n",
    "    dff = apply_filters(df.copy(), tt_sel, topic_sel, t_range)\n",
    "    total_docs = len(dff)\n",
    "\n",
    "    # Color por ThinkTank o por Tópico\n",
    "    if color_mode == \"tt\":\n",
    "        color_col = \"ThinkTank\"\n",
    "        cmap = None\n",
    "    else:\n",
    "        color_col = \"topic_str\"\n",
    "        cmap = topic_cmap\n",
    "\n",
    "    dff[\"topic_str\"] = dff[\"topic_id\"].astype(str)\n",
    "\n",
    "    # Hover\n",
    "    hover3d = (\"<b>%{{customdata[4]}}</b><br>\"\n",
    "               \"<b>Topic</b>: %{{customdata[0]}} — %{{customdata[1]}}<br>\"\n",
    "               \"<b>Think Tank</b>: %{{customdata[2]}}<br>\"\n",
    "               \"<b>Fecha</b>: %{{customdata[3]|%Y-%m-%d}}\")\n",
    "    cd = np.stack([\n",
    "        dff[\"topic_str\"].astype(str).values,\n",
    "        dff[\"topic_kw\"].astype(str).values,\n",
    "        dff[\"ThinkTank\"].astype(str).values,\n",
    "        dff[\"FechaPublicacion\"].values.astype(\"datetime64[ns]\"),\n",
    "        dff[\"ID\"].astype(str).values\n",
    "    ], axis=1) if not dff.empty else np.empty((0,5), dtype=object)\n",
    "\n",
    "    fig3d = px.scatter_3d(\n",
    "        dff, x=\"x3\", y=\"y3\", z=\"z3\", color=color_col,\n",
    "        color_discrete_map=cmap if cmap else None,\n",
    "        title=f\"UMAP 3D — documentos filtrados (N={{total_docs}})\"\n",
    "    )\n",
    "    fig3d.update_traces(marker=dict(size=SIZE3D, opacity=0.85), hovertemplate=hover3d, customdata=cd)\n",
    "\n",
    "    hover2d = (\"<b>%{{customdata[4]}}</b><br>\"\n",
    "               \"<b>Topic</b>: %{{customdata[0]}} — %{{customdata[1]}}<br>\"\n",
    "               \"<b>Think Tank</b>: %{{customdata[2]}}<br>\"\n",
    "               \"<b>Fecha</b>: %{{customdata[3]|%Y-%m-%d}}\")\n",
    "    fig2d = px.scatter(\n",
    "        dff, x=\"x\", y=\"y\", color=color_col,\n",
    "        color_discrete_map=cmap if cmap else None,\n",
    "        title=\"UMAP 2D\"\n",
    "    )\n",
    "    fig2d.update_traces(marker=dict(size=SIZE2D, opacity=0.85), hovertemplate=hover2d, customdata=cd)\n",
    "    fig2d.update_layout(legend=dict(itemsizing=\"constant\"))\n",
    "\n",
    "    # Serie mensual por ThinkTank\n",
    "    if dff.empty:\n",
    "        fig_series = go.Figure(); fig_series.update_layout(title=\"Documentos por mes (sin datos)\")\n",
    "    else:\n",
    "        dff[\"period\"] = dff[\"FechaPublicacion\"].dt.to_period(\"M\").astype(str)\n",
    "        grp = dff.groupby([\"period\",\"ThinkTank\"]).size().reset_index(name=\"n\")\n",
    "        fig_series = px.bar(\n",
    "            grp, x=\"period\", y=\"n\", color=\"ThinkTank\",\n",
    "            title=f\"Documentos por mes — total N={{total_docs}}\",\n",
    "            barmode=\"stack\"\n",
    "        )\n",
    "        fig_series.update_layout(xaxis={{\"categoryorder\":\"category ascending\"}})\n",
    "\n",
    "    # Tabla\n",
    "    def kw_for(tid):\n",
    "        ks = kw_by_topic.get(str(int(tid))) or kw_by_topic.get(int(tid)) or []\n",
    "        return \", \".join(ks[:KW_FIXED])\n",
    "    table_df = dff[[\"topic_id\",\"ThinkTank\",\"FechaPublicacion\",\"Enlace\",\"ID\"]].copy()\n",
    "    table_df[\"keywords\"] = dff[\"topic_id\"].apply(kw_for)\n",
    "    table_df[\"FechaPublicacion\"] = table_df[\"FechaPublicacion\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    # enlace clickeable en markdown\n",
    "    def mk(url, idx):\n",
    "        u = str(url) if pd.notna(url) else \"\"\n",
    "        if u.startswith(\"http\"):\n",
    "            return f\"[abrir]({{u}})\"\n",
    "        return \"\"\n",
    "    table_df[\"Enlace_markdown\"] = [mk(u, i) for i,u in enumerate(table_df[\"Enlace\"].tolist())]\n",
    "    table_df = table_df[[\"topic_id\",\"keywords\",\"ThinkTank\",\"FechaPublicacion\",\"Enlace_markdown\"]]\n",
    "    table_data = table_df.to_dict(\"records\")\n",
    "\n",
    "    banner = banner_keywords(topic_sel)\n",
    "    return fig3d, fig2d, fig_series, table_data, banner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=False)\n",
    "'''\n",
    "    with open(app_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(app_code)\n",
    "    _log(f\"App escrita en: {app_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Orquestación principal\n",
    "# =========================\n",
    "def run_pipeline_on_df(df: pd.DataFrame, cfg: Config = CFG) -> Dict[str, object]:\n",
    "    start_run_dirs(cfg)\n",
    "\n",
    "    # columnas requeridas\n",
    "    need = [cfg.id_col, cfg.cat_col, cfg.text_col, cfg.date_col, cfg.url_col]\n",
    "    if not set(need).issubset(df.columns):\n",
    "        missing = set(need) - set(df.columns)\n",
    "        raise KeyError(f\"Faltan columnas: {missing}\")\n",
    "\n",
    "    # copia + tipos\n",
    "    df_local = df[need].copy()\n",
    "    # normaliza fecha\n",
    "    if not np.issubdtype(df_local[cfg.date_col].dtype, np.datetime64):\n",
    "        df_local[cfg.date_col] = pd.to_datetime(df_local[cfg.date_col], errors=\"coerce\")\n",
    "\n",
    "    # filtro temporal (si fue seteado)\n",
    "    if cfg.date_start is not None or cfg.date_end is not None:\n",
    "        m = pd.Series(True, index=df_local.index)\n",
    "        if cfg.date_start is not None:\n",
    "            m &= (df_local[cfg.date_col] >= cfg.date_start)\n",
    "        if cfg.date_end is not None:\n",
    "            m &= (df_local[cfg.date_col] < cfg.date_end)\n",
    "        df_local = df_local.loc[m].copy()\n",
    "        _log(f\"Filtro fechas aplicado. Rango: {cfg.date_start} → {cfg.date_end} (exclusivo)\")\n",
    "\n",
    "    df_local[cfg.id_col] = df_local[cfg.id_col].astype(str)\n",
    "    df_local[cfg.cat_col] = df_local[cfg.cat_col].astype(str)\n",
    "    df_local[cfg.text_col] = df_local[cfg.text_col].astype(str)\n",
    "    df_local[cfg.url_col] = df_local[cfg.url_col].astype(str)\n",
    "    df_local = df_local.reset_index(drop=True)\n",
    "\n",
    "    texts_raw = df_local[cfg.text_col].tolist()\n",
    "    _log(f\"N documentos: {len(texts_raw)}\")\n",
    "\n",
    "    # Embeddings\n",
    "    emb = compute_embeddings(df_local, cfg)\n",
    "\n",
    "    # Rejilla UMAP + BERTopic (elegimos por menos ruido y más #clusters)\n",
    "    best = None\n",
    "    texts_clean = [scrub_text(t, cfg.min_token_len) for t in texts_raw]\n",
    "    for nn in cfg.umap_neighbors_grid:\n",
    "        for md in cfg.umap_min_dist_grid:\n",
    "            _log(f\"UMAP grid → n_neighbors={nn}, min_dist={md}\")\n",
    "            tm = make_topic_model(len(texts_clean), cfg, nn, md)\n",
    "            topics, _ = tm.fit_transform(texts_clean, embeddings=emb)\n",
    "            topics = np.asarray(topics, dtype=int)\n",
    "\n",
    "            # métricas\n",
    "            noise_frac = float((topics==-1).mean()) if len(topics) else 1.0\n",
    "            n_clusters = len([t for t in np.unique(topics) if t!=-1])\n",
    "\n",
    "            # score: maximizamos clusters y minimizamos ruido\n",
    "            score = (n_clusters, 1.0 - noise_frac)\n",
    "            if (best is None) or (score > best[\"score\"]):\n",
    "                best = {\"tm\": tm, \"topics\": topics, \"score\": score,\n",
    "                        \"n_neighbors\": nn, \"min_dist\": md}\n",
    "\n",
    "    tm = best[\"tm\"]; topics = best[\"topics\"]\n",
    "    _log(f\"Mejor combinación → n_neighbors={best['n_neighbors']}, min_dist={best['min_dist']}\")\n",
    "\n",
    "    # Keywords macro\n",
    "    kw_macro = fixed_keywords_macro(tm, cfg.topic_kw_fixed)\n",
    "\n",
    "    # --- Micro-tópicos / Singletons sobre ruido (-1)\n",
    "    idx_noise = np.where(topics == -1)[0]\n",
    "    micro_clusters = build_micro_clusters(idx_noise, emb, cfg)\n",
    "\n",
    "    # Asignar IDs nuevos\n",
    "    cur_max = max([-1]+[int(t) for t in np.unique(topics) if t!=-1])\n",
    "    new_id = cur_max + 1\n",
    "    micro_kw: Dict[int, List[str]] = {}\n",
    "    for cid, rows in micro_clusters.items():\n",
    "        rows = list(rows)\n",
    "        topics[rows] = new_id\n",
    "        # keywords locales TF-IDF\n",
    "        cluster_texts = [texts_clean[i] for i in rows]\n",
    "        micro_kw[new_id] = tfidf_keywords_for_docs(cluster_texts, cfg.topic_kw_fixed, cfg.tfidf_max_features_micro)\n",
    "        new_id += 1\n",
    "\n",
    "    # Unimos keywords (macro + micro)\n",
    "    kw_all = dict(kw_macro)\n",
    "    kw_all.update(micro_kw)\n",
    "\n",
    "    # Mapeo topic → ThinkTank top-1\n",
    "    topic_to_idx: Dict[int, List[int]] = defaultdict(list)\n",
    "    for i, t in enumerate(topics.tolist()):\n",
    "        topic_to_idx[int(t)].append(i)\n",
    "    rows = []\n",
    "    for t, idxs in topic_to_idx.items():\n",
    "        tt_counts = Counter(df_local.loc[idxs, cfg.cat_col].tolist())\n",
    "        top_tt = tt_counts.most_common(1)[0][0] if len(tt_counts)>0 else \"(sin docs)\"\n",
    "        rows.append({\"topic_id\": t, \"category_top1\": top_tt})\n",
    "    df_map = pd.DataFrame(rows)\n",
    "    df_map[\"topic_desc_fixed\"] = df_map[\"topic_id\"].map(lambda tid: \", \".join(kw_all.get(int(tid), [])))\n",
    "    Path(cfg.out_dir_map).mkdir(parents=True, exist_ok=True)\n",
    "    df_map.to_csv(os.path.join(cfg.out_dir_map, \"topic_to_category_top1.csv\"), index=False)\n",
    "\n",
    "    # Export docs-topics\n",
    "    df_docs = pd.DataFrame({\n",
    "        \"doc_row\": np.arange(len(texts_raw)),\n",
    "        \"ID\": df_local[cfg.id_col].values,\n",
    "        \"Think_Tank_true\": df_local[cfg.cat_col].values,\n",
    "        \"FechaPublicacion\": df_local[cfg.date_col].values,\n",
    "        \"topic_id\": topics\n",
    "    })\n",
    "    df_docs.to_csv(os.path.join(cfg.out_dir_topics, \"docs_topics.csv\"), index=False)\n",
    "\n",
    "    # UMAP 2D/3D para App\n",
    "    _log(\"UMAP 2D/3D para visual…\")\n",
    "    reducer2d = umap.UMAP(n_neighbors=best[\"n_neighbors\"], min_dist=0.0, n_components=2,\n",
    "                          metric=cfg.umap_metric, random_state=cfg.random_state, verbose=True)\n",
    "    z2d = reducer2d.fit_transform(emb)\n",
    "    reducer3d = umap.UMAP(n_neighbors=best[\"n_neighbors\"], min_dist=0.0, n_components=3,\n",
    "                          metric=cfg.umap_metric, random_state=cfg.random_state, verbose=True)\n",
    "    z3d = reducer3d.fit_transform(emb)\n",
    "\n",
    "    # df_vis para App\n",
    "    date_series = pd.to_datetime(df_local[cfg.date_col], errors=\"coerce\").dt.floor(\"D\")\n",
    "    topic_str = np.array([str(int(t)) for t in topics])\n",
    "    df_vis = pd.DataFrame({\n",
    "        \"x\": z2d[:,0], \"y\": z2d[:,1],\n",
    "        \"x3\": z3d[:,0], \"y3\": z3d[:,1], \"z3\": z3d[:,2],\n",
    "        \"topic_id\": topics,\n",
    "        \"topic_str\": topic_str,\n",
    "        \"ThinkTank\": df_local[cfg.cat_col].values,\n",
    "        \"FechaPublicacion\": date_series,\n",
    "        \"ID\": df_local[cfg.id_col].values,\n",
    "        \"Enlace\": df_local[cfg.url_col].values\n",
    "    })\n",
    "    df_vis[\"topic_kw\"] = [\", \".join(kw_all.get(int(t), [])) for t in df_vis[\"topic_id\"]]\n",
    "\n",
    "    # Guardar también df_vis (con fecha completa), y topic_info macro\n",
    "    tm.get_topic_info().to_csv(os.path.join(cfg.out_dir_topics, \"topic_info_macro.csv\"), index=False)\n",
    "    df_vis.to_csv(os.path.join(cfg.out_dir_topics, \"df_vis.csv\"), index=False)\n",
    "\n",
    "    # App\n",
    "    write_dash_app(df_vis, kw_all, cfg.run_dir, kw_fixed=cfg.topic_kw_fixed,\n",
    "                   size2d=cfg.app_marker_2d_size, size3d=cfg.app_marker_3d_size)\n",
    "\n",
    "    _log(\"Listo ✓\")\n",
    "    return {\n",
    "        \"run_dir\": cfg.run_dir,\n",
    "        \"embeddings\": emb,\n",
    "        \"topic_model_macro\": tm,\n",
    "        \"topics_final\": topics,\n",
    "        \"keywords_all\": kw_all,\n",
    "        \"df_vis\": df_vis\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7ff75",
   "metadata": {},
   "source": [
    "# Fin plebiscito 1 - Fin plebscito 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c62cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:39:50] [RUN] id=20251021_153950 -> NLP\\runs\\20251021_153950\n",
      "[15:39:50] Filtro fechas aplicado. Rango: 2019-01-01 00:00:00 → 2023-12-31 00:00:00 (exclusivo)\n",
      "[15:39:50] N documentos: 17418\n",
      "[15:39:50] Calculando embeddings SBERT…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000025B8362FA70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 796, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1535, in enumerate\n",
      "    def enumerate():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85edffa63f2a454d8a3c69fa9b5cb0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m CFG.force_reembed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 3) Corre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[43mrun_pipeline_on_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 638\u001b[39m, in \u001b[36mrun_pipeline_on_df\u001b[39m\u001b[34m(df, cfg)\u001b[39m\n\u001b[32m    635\u001b[39m _log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mN documentos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts_raw)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# Embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m emb = \u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_local\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[38;5;66;03m# Rejilla UMAP + BERTopic (elegimos por menos ruido y más #clusters)\u001b[39;00m\n\u001b[32m    641\u001b[39m best = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mcompute_embeddings\u001b[39m\u001b[34m(df_local, cfg)\u001b[39m\n\u001b[32m    150\u001b[39m model = SentenceTransformer(cfg.sbert_model_name)\n\u001b[32m    151\u001b[39m texts = df_local[cfg.text_col].astype(\u001b[38;5;28mstr\u001b[39m).map(\u001b[38;5;28;01mlambda\u001b[39;00m x: scrub_text(x, cfg.min_token_len)).tolist()\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m emb = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msbert_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_numpy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sk_normalize(emb.astype(np.float32))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1087\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1085\u001b[39m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[32m   1086\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m                 embeddings = \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1089\u001b[39m         all_embeddings.extend(embeddings)\n\u001b[32m   1091\u001b[39m all_embeddings = [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1) Ajusta rango temporal si quieres\n",
    "CFG.date_start = pd.Timestamp(\"2019-01-01\")\n",
    "CFG.date_end   = pd.Timestamp(\"2023-12-31\")  # exclusivo\n",
    "\n",
    "# 2) Fuerza recrear embeddings (ignora cualquier cache previo)\n",
    "CFG.force_reembed = True\n",
    "\n",
    "# 3) Corre\n",
    "results = run_pipeline_on_df(df, CFG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a017f",
   "metadata": {},
   "source": [
    "# Estallido social - Fin plebiscito"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
